{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red\"><b>Disclaimer of Liability</b></h1>\n",
    "\n",
    "**The material and information contained on this website is for general information, reference, and self-learning purposes only. You should not rely upon the material or information on the website as a basis for making any academic, business, legal or any other decisions. You should not copy any material or information on the website into any of your academic, business, legal or any other non-private usages. ZHANG Wengyu will not be responsible for any consequences due to your violations.**\n",
    "\n",
    "\n",
    "Whilst ZHANG Wengyu endeavours to keep the information up to date and correct, ZHANG Wengyu makes no representations or warranties of any kind, express or implied about the completeness, accuracy, reliability, suitability or availability with respect to the website or the information, products, services or related graphics contained on the website for any purpose. Any reliance you place on such material is therefore strictly at your own risk.\n",
    "\n",
    "\n",
    "ZHANG Wengyu will not be liable for any false, inaccurate, inappropriate or incomplete information presented on the website.\n",
    "\n",
    "\n",
    "Although every effort is made to keep the website up and running smoothly, due to the nature of the Internet and the technology involved, ZHANG Wengyu takes no responsibility for and will not be liable for the website being temporarily unavailable due to technical issues (or otherwise) beyond its control or for any loss or damage suffered as a result of the use of or access to, or inability to use or access this website whatsoever.\n",
    "\n",
    "\n",
    "Certain links in this website will lead to websites which are not under the control of ZHANG Wengyu. When you activate these you will leave ZHANG Wengyu's  website. ZHANG Wengyu has no control over and accepts no liability in respect of materials, products or services available on any website which is not under the control of ZHANG Wengyu.\n",
    "\n",
    "\n",
    "To the extent not prohibited by law, in no circumstances shall ZHANG Wengyu be liable to you or any other third parties for any loss or damage (including, without limitation, damage for loss of business or loss of profits) arising directly or indirectly from your use of or inability to use, this site or any of the material contained in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advancing House Price Predictions: Data to Model Insights\n",
    "Group Name: Pangolin \n",
    "\n",
    "JIANG Yiyang\n",
    "\n",
    "ZHANG Wengyu\n",
    "\n",
    "Deparment of Computing, The Hong Kong Polytechnic University\n",
    "\n",
    "Nov. 2023\n",
    "\n",
    "References:\n",
    "- erikbruin, \"House prices: Lasso, XGBoost, and a detailed EDA,\" *Kaggle.com*, May 10, 2018. https://www.kaggle.com/code/erikbruin/house-prices-lasso-xgboost-and-a-detailed-eda.\n",
    "- lavanyashukla01, \"How I made top 0.3% on a Kaggle competition,\" *Kaggle.com*, Jun. 09, 2019. https://www.kaggle.com/code/lavanyashukla01/how-i-made-top-0-3-on-a-kaggle-competition.\n",
    "â€Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "from datetime import datetime\n",
    "from scipy.stats import yeojohnson\n",
    "from scipy.stats import skew \n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import skew, boxcox_normmax\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "train.shape, test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ID = train['Id']\n",
    "test_ID = test['Id']\n",
    "# Now drop the  'Id' colum since it's unnecessary for  the prediction process.\n",
    "train.drop(['Id'], axis=1, inplace=True)\n",
    "test.drop(['Id'], axis=1, inplace=True)\n",
    "\n",
    "# Deleting outliers\n",
    "train = train[train.GrLivArea < 4500]\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\n",
    "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
    "y = train.SalePrice.reset_index(drop=True)\n",
    "train_features = train.drop(['SalePrice'], axis=1)\n",
    "test_features = test\n",
    "\n",
    "features = pd.concat([train_features, test_features]).reset_index(drop=True)\n",
    "print(features.shape)\n",
    "# Some of the non-numeric predictors are stored as numbers; we convert them into strings \n",
    "features['MSSubClass'] = features['MSSubClass'].apply(str)\n",
    "features['YrSold'] = features['YrSold'].astype(str)\n",
    "features['MoSold'] = features['MoSold'].astype(str)\n",
    "\n",
    "features['Functional'] = features['Functional'].fillna('Normal')\n",
    "features['Electrical'] = features['Electrical'].fillna(\"SBrkr\")\n",
    "features['KitchenQual'] = features['KitchenQual'].fillna(\"TA\")\n",
    "features['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])\n",
    "features['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\n",
    "features['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])\n",
    "\n",
    "features[\"PoolQC\"] = features[\"PoolQC\"].fillna(\"None\")\n",
    "\n",
    "for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n",
    "    features[col] = features[col].fillna(0)\n",
    "for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n",
    "    features[col] = features[col].fillna('None')\n",
    "for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n",
    "    features[col] = features[col].fillna('None')\n",
    "\n",
    "features['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "\n",
    "objects = []\n",
    "for i in features.columns:\n",
    "    if features[i].dtype == object:\n",
    "        objects.append(i)\n",
    "\n",
    "features.update(features[objects].fillna('None'))\n",
    "\n",
    "features['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "# Filling in the rest of the NA's\n",
    "numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numerics = []\n",
    "for i in features.columns:\n",
    "    if features[i].dtype in numeric_dtypes:\n",
    "        numerics.append(i)\n",
    "features.update(features[numerics].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numerics2 = []\n",
    "for i in features.columns:\n",
    "    if features[i].dtype in numeric_dtypes:\n",
    "        numerics2.append(i)\n",
    "\n",
    "skew_features = features[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "\n",
    "high_skew = skew_features[skew_features > 0.5]\n",
    "skew_index = high_skew.index\n",
    "\n",
    "# for i in skew_index:\n",
    "#     features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1))\n",
    "\n",
    "for i in skew_index:\n",
    "    features[i] = np.log1p(features[i])\n",
    "\n",
    "# for i in skew_index:\n",
    "#     features[i], _ = yeojohnson(features[i]+1) \n",
    "\n",
    "\n",
    "sns.distplot(features['LotArea'], kde = True, hist=True, fit = norm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.drop(['Utilities', 'Street', 'PoolQC',], axis=1)\n",
    "\n",
    "features['YrBltAndRemod']=features['YearBuilt']+features['YearRemodAdd']\n",
    "features['TotalSF']=features['TotalBsmtSF'] + features['1stFlrSF'] + features['2ndFlrSF']\n",
    "\n",
    "features['Total_sqr_footage'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +\n",
    "                                 features['1stFlrSF'] + features['2ndFlrSF'])\n",
    "\n",
    "features['Total_Bathrooms'] = (features['FullBath'] + (0.5 * features['HalfBath']) +\n",
    "                               features['BsmtFullBath'] + (0.5 * features['BsmtHalfBath']))\n",
    "\n",
    "features['Total_porch_sf'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n",
    "                              features['EnclosedPorch'] + features['ScreenPorch'] +\n",
    "                              features['WoodDeckSF'])\n",
    "\n",
    "# simplified features\n",
    "features['haspool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n",
    "features['has2ndfloor'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "features['hasgarage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n",
    "features['hasbsmt'] = features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "features['hasfireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "print(features.shape)\n",
    "final_features = pd.get_dummies(features).reset_index(drop=True)\n",
    "print(final_features.shape)\n",
    "\n",
    "X = final_features.iloc[:len(y), :]\n",
    "X_sub = final_features.iloc[len(X):, :]\n",
    "\n",
    "print('X', X.shape, 'y', y.shape, 'X_sub', X_sub.shape)\n",
    "\n",
    "outliers = [30, 88, 462, 631, 1322]\n",
    "X = X.drop(X.index[outliers])\n",
    "y = y.drop(y.index[outliers])\n",
    "\n",
    "overfit = []\n",
    "for i in X.columns:\n",
    "    counts = X[i].value_counts()\n",
    "    zeros = counts.iloc[0]\n",
    "    if zeros / len(X) * 100 > 99.94:\n",
    "        overfit.append(i)\n",
    "\n",
    "overfit = list(overfit)\n",
    "overfit.append('MSZoning_C (all)')\n",
    "\n",
    "X = X.drop(overfit, axis=1).copy()\n",
    "X_sub = X_sub.drop(overfit, axis=1).copy()\n",
    "\n",
    "print('X', X.shape, 'y', y.shape, 'X_sub', X_sub.shape)\n",
    "              \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# rmsle\n",
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "\n",
    "# build our model scoring function\n",
    "def cv_rmse(model, X=X):\n",
    "    rmse = np.sqrt(-cross_val_score(model, X, y,\n",
    "                                    scoring=\"neg_mean_squared_error\",\n",
    "                                    cv=kfolds))\n",
    "    return (rmse)\n",
    "\n",
    "# setup models    \n",
    "alphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\n",
    "alphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n",
    "e_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\n",
    "e_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n",
    "\n",
    "ridge = make_pipeline(RobustScaler(),\n",
    "                      RidgeCV(alphas=alphas_alt, cv=kfolds,))\n",
    "\n",
    "lasso = make_pipeline(RobustScaler(),\n",
    "                      LassoCV(max_iter=int(1e7), alphas=alphas2,\n",
    "                              random_state=42, cv=kfolds))\n",
    "\n",
    "elasticnet = make_pipeline(RobustScaler(),\n",
    "                           ElasticNetCV(max_iter=int(1e7), alphas=e_alphas,\n",
    "                                        cv=kfolds, random_state=42, l1_ratio=e_l1ratio))\n",
    "                                        \n",
    "svr = make_pipeline(RobustScaler(),\n",
    "                      SVR(C= 20, epsilon= 0.008, gamma=0.0003,))\n",
    "\n",
    "\n",
    "gbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =42)\n",
    "                                   \n",
    "\n",
    "lightgbm = LGBMRegressor(objective='regression', \n",
    "                                       num_leaves=4,\n",
    "                                       learning_rate=0.01, \n",
    "                                       n_estimators=5000,\n",
    "                                       max_bin=200, \n",
    "                                       bagging_fraction=0.75,\n",
    "                                       bagging_freq=5, \n",
    "                                       bagging_seed=7,\n",
    "                                       feature_fraction=0.2,\n",
    "                                       feature_fraction_seed=7,\n",
    "                                       verbose=-1,\n",
    "                                       #min_data_in_leaf=2,\n",
    "                                       #min_sum_hessian_in_leaf=11\n",
    "                                       )\n",
    "                                       \n",
    "\n",
    "xgboost = XGBRegressor(learning_rate=0.01, n_estimators=3460,\n",
    "                                     max_depth=3, min_child_weight=0,\n",
    "                                     gamma=0, subsample=0.7,\n",
    "                                     colsample_bytree=0.7,\n",
    "                                     objective='reg:linear', nthread=-1,\n",
    "                                     scale_pos_weight=1, seed=27,\n",
    "                                     reg_alpha=0.00006, random_state=42)\n",
    "\n",
    "# stack\n",
    "stack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet,\n",
    "                                            gbr, xgboost, lightgbm),\n",
    "                                meta_regressor=xgboost,\n",
    "                                use_features_in_secondary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TEST score on CV')\n",
    "\n",
    "score = cv_rmse(ridge)\n",
    "print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n",
    "score = cv_rmse(lasso)\n",
    "print(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n",
    "score = cv_rmse(elasticnet)\n",
    "print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n",
    "score = cv_rmse(svr)\n",
    "print(\"SVR score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n",
    "score = cv_rmse(lightgbm)\n",
    "print(\"Lightgbm score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n",
    "score = cv_rmse(gbr)\n",
    "print(\"GradientBoosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n",
    "score = cv_rmse(xgboost)\n",
    "print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('START Fit')\n",
    "print(datetime.now(), 'StackingCVRegressor')\n",
    "stack_gen_model = stack_gen.fit(np.array(X), np.array(y))\n",
    "print(datetime.now(), 'elasticnet')\n",
    "elastic_model_full_data = elasticnet.fit(X, y)\n",
    "print(datetime.now(), 'lasso')\n",
    "lasso_model_full_data = lasso.fit(X, y)\n",
    "print(datetime.now(), 'ridge')\n",
    "ridge_model_full_data = ridge.fit(X, y)\n",
    "print(datetime.now(), 'svr')\n",
    "svr_model_full_data = svr.fit(X, y)\n",
    "print(datetime.now(), 'GradientBoosting')\n",
    "gbr_model_full_data = gbr.fit(X, y)\n",
    "print(datetime.now(), 'xgboost')\n",
    "xgb_model_full_data = xgboost.fit(X, y)\n",
    "print(datetime.now(), 'lightgbm')\n",
    "lgb_model_full_data = lightgbm.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend_models_predict(X):\n",
    "    return ((0.1 * elastic_model_full_data.predict(X)) + \\\n",
    "            (0.05 * lasso_model_full_data.predict(X)) + \\\n",
    "            (0.1 * ridge_model_full_data.predict(X)) + \\\n",
    "            (0.1 * svr_model_full_data.predict(X)) + \\\n",
    "            (0.1 * gbr_model_full_data.predict(X)) + \\\n",
    "            (0.15 * xgb_model_full_data.predict(X)) + \\\n",
    "            (0.1 * lgb_model_full_data.predict(X)) + \\\n",
    "            (0.3 * stack_gen_model.predict(np.array(X))))\n",
    "\n",
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "submission.iloc[:,1] = np.floor(np.expm1(blend_models_predict(np.array(X_sub))))\n",
    "q1 = submission['SalePrice'].quantile(0.0042)\n",
    "q2 = submission['SalePrice'].quantile(0.99)\n",
    "\n",
    "submission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\n",
    "submission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\n",
    "\n",
    "submission.to_csv(\"./House_price_submission_blend_v7.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend_models_predict(X):\n",
    "    return ((0.1 * elastic_model_full_data.predict(X)) + \\\n",
    "            (0.05 * lasso_model_full_data.predict(X)) + \\\n",
    "            (0.1 * ridge_model_full_data.predict(X)) + \\\n",
    "            (0.1 * svr_model_full_data.predict(X)) + \\\n",
    "            (0.1 * gbr_model_full_data.predict(X)) + \\\n",
    "            (0.15 * xgb_model_full_data.predict(X)) + \\\n",
    "            (0.1 * lgb_model_full_data.predict(X)) + \\\n",
    "            (0.3 * stack_gen_model.predict(np.array(X))))\n",
    "\n",
    "print(rmsle(y, blend_models_predict(X)))\n",
    "\n",
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "sub_1 = pd.read_csv('House_price_submission_blend_v6.csv')\n",
    "sub_2 = pd.read_csv('House_price_submission_blend_v7.csv')\n",
    "sub_3 = pd.read_csv('House_price_submission_blend_v8.csv')\n",
    "\n",
    "submission.iloc[:,1] = np.floor((0.7 * sub_1.iloc[:,1]) + \n",
    "                                (0.2 * sub_2.iloc[:,1]) + \n",
    "                                (0.1 * sub_3.iloc[:,1]) \n",
    "                                )\n",
    "\n",
    "\n",
    "submission.to_csv(\"House_price_submission_blend2_v10.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multiple Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os, sys, math\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from scipy.stats import skew, norm\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax\n",
    "\n",
    "# Data preprocessing\n",
    "train_df=pd.read_csv(\".train.csv\")\n",
    "\n",
    "text_df=pd.read_csv(\".test.csv\")\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data, valid_data = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Separate the target variable from training and validation data\n",
    "y_train = train_data.pop('SalePrice')/100000\n",
    "y_valid = valid_data.pop('SalePrice')/100000\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_features = train_data.select_dtypes(include=['float64', 'int64']).columns.drop(['Id']).tolist()\n",
    "categorical_features = train_data.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Build transformers for the numeric and categorical columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Create the preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = preprocessor.fit_transform(train_data)\n",
    "X_valid = preprocessor.transform(valid_data)\n",
    "X_test = preprocessor.transform(text_df)\n",
    "\n",
    "print(X_train.shape, X_valid.shape, X_test.shape)\n",
    "\n",
    "X_train = X_train.toarray()\n",
    "X_valid = X_valid.toarray()\n",
    "X_test  = X_test.toarray()\n",
    "y_valid = y_valid.to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "# MLP Model Build\n",
    "class RMSELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss,self).__init__()\n",
    "\n",
    "    def forward(self,x,y):\n",
    "        criterion = nn.MSELoss()\n",
    "        eps = 1e-6\n",
    "        loss = torch.sqrt(criterion(x, y) + eps)\n",
    "        return loss\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, indim, blockNum = 2, scale=4, dropout=0.2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.indim = indim\n",
    "        self.dim = self.indim * scale\n",
    "        self.infc = nn.Sequential(\n",
    "            nn.Linear(self.indim, self.dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        def create_block():\n",
    "            block = nn.Sequential(\n",
    "                nn.Linear(self.dim, self.dim),\n",
    "                nn.BatchNorm1d(self.dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dropout),\n",
    "\n",
    "                nn.Linear(self.dim, self.dim),\n",
    "                nn.BatchNorm1d(self.dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dropout),\n",
    "\n",
    "                nn.Linear(self.dim, self.dim),\n",
    "                nn.BatchNorm1d(self.dim)\n",
    "            )\n",
    "            return block\n",
    "\n",
    "        self.blocks = nn.ModuleList([create_block() for _ in range(blockNum)])\n",
    "        self.out = nn.Linear(self.dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float32)\n",
    "        x = self.infc(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x) + x  # Add the output of the block to the input (residual connection)\n",
    "            x = F.relu(x)     # Apply ReLU activation function\n",
    "        \n",
    "        out = self.out(x)   \n",
    "\n",
    "        return out \n",
    "    \n",
    "model = MLP(301).cuda()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, lr, total_epochs, schedule=None, if_cos=True):\n",
    "    \"\"\"Decay the learning rate based on schedule\"\"\"\n",
    "    if if_cos:  # cosine lr schedule\n",
    "        lr *= 0.5 * (1.0 + math.cos(math.pi * epoch / total_epochs))\n",
    "    else:  # stepwise lr schedule\n",
    "        for milestone in schedule:\n",
    "            lr *= 0.1 if epoch >= milestone else 1.0\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "        \n",
    "\n",
    "def testModel():\n",
    "    total_val_loss = 0\n",
    "    for X_batch_val, y_batch_val in valid_loader:\n",
    "        # Move data to device\n",
    "        X_batch_val = X_batch_val.to(device)\n",
    "        y_batch_val = y_batch_val.to(device)\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        # Forward pass\n",
    "        x_val = X_batch_val\n",
    "        \n",
    "        predictions_val = model(x_val)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss_test = testLoss(predictions_val, y_batch_val.unsqueeze(1))\n",
    "        \n",
    "        total_val_loss += loss_test.item()\n",
    "    return total_val_loss/len(valid_loader)\n",
    "           \n",
    "\n",
    "# Loss function\n",
    "criterion = RMSELoss().cuda()\n",
    "\n",
    "testLoss = RMSELoss().cuda()\n",
    "\n",
    "lr = 0.001\n",
    "# # Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "X_valid_tensor = torch.tensor(X_valid, dtype=torch.float32)\n",
    "y_valid_tensor = torch.tensor(y_valid, dtype=torch.float32)\n",
    "\n",
    "valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "test_dataset = TensorDataset(X_test_tensor)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 300\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = 100000\n",
    "best_train_loss = 100000\n",
    "best_model = None\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        \n",
    "        if epoch >= 100:\n",
    "            adjust_learning_rate(optimizer, epoch, lr, num_epochs-100)\n",
    "        \n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        # Forward pass\n",
    "        x = X_batch\n",
    "        \n",
    "        \n",
    "        predictions = model(x)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(predictions, y_batch.unsqueeze(1))\n",
    "        \n",
    "        # Zero out any gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    if avg_loss < best_train_loss:\n",
    "        best_train_loss = avg_loss\n",
    "    \n",
    "    test_loss = testModel()\n",
    "    \n",
    "    if test_loss < best_val_loss:\n",
    "        best_val_loss = test_loss\n",
    "        best_model = model\n",
    "    \n",
    "    val_losses.append(test_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_loss:.6f}, Test Loss: {test_loss:.6f}\")\n",
    "    \n",
    "plt.figure(figsize = (12,5))\n",
    "plt.plot(range (0, num_epochs), train_losses, label='Train')\n",
    "plt.plot(range (0, num_epochs), val_losses, label='Vaild')\n",
    "plt.legend() \n",
    "\n",
    "# Test the mode\n",
    "best_model.eval()  # Set the model to evaluation mode\n",
    "test_predictions = best_model(X_test_tensor.cuda())\n",
    "\n",
    "test_predictions*=100000\n",
    "\n",
    "test_arr = test_predictions.squeeze(-1).cpu().detach().numpy()\n",
    "\n",
    "text_df=pd.read_csv(\"./test.csv\")\n",
    "submission_transformer = pd.DataFrame({'Id': text_df['Id'], 'SalePrice': test_arr})\n",
    "submission_transformer.to_csv('./MLP_submssion.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, MultiHeadAttention, Flatten, LeakyReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "# Load the provided datasets again\n",
    "train_data_provided = pd.read_csv(\"train.csv\")\n",
    "test_data_provided = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Remove the 'Id' column as it's not a feature for prediction\n",
    "train_ids = train_data_provided.pop('Id')\n",
    "test_ids = test_data_provided.pop('Id')\n",
    "\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data, valid_data = train_test_split(train_data_provided, test_size=0.2, random_state=42)\n",
    "\n",
    "# Separate the target variable from training and validation data\n",
    "y_train = train_data.pop('SalePrice')\n",
    "y_valid = valid_data.pop('SalePrice')\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_features = train_data.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "categorical_features = train_data.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Build transformers for the numeric and categorical columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Create the preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = preprocessor.fit_transform(train_data)\n",
    "X_valid = preprocessor.transform(valid_data)\n",
    "X_test_provided = preprocessor.transform(test_data_provided)\n",
    "\n",
    "X_train.shape, X_valid.shape, X_test_provided.shape\n",
    "\n",
    "\n",
    "# Convert the sparse matrices to dense numpy arrays\n",
    "X_train = X_train.toarray()\n",
    "X_valid = X_valid.toarray()\n",
    "X_test_provided  = X_test_provided.toarray()\n",
    "\n",
    "def RMSE(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    return tf.keras.backend.sqrt(tf.keras.backend.mean(tf.keras.backend.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "def simpler_transformer_block(inputs, node_size):\n",
    "    # Expand the dimension to simulate a sequence\n",
    "    expanded_inputs = tf.keras.layers.Reshape((-1, 1))(inputs)\n",
    "    \n",
    "    # Multi-head self-attention with fewer heads\n",
    "    attn_output = MultiHeadAttention(num_heads=2, key_dim=node_size)(expanded_inputs, expanded_inputs)\n",
    "    return attn_output\n",
    "\n",
    "def build_simpler_transformer_model(input_dim):\n",
    "    inputs = tf.keras.Input(shape=(input_dim,))\n",
    "    x = Dense(32, kernel_initializer='he_normal')(inputs)\n",
    "    x = LeakyReLU()(x)  # Use LeakyReLU activation\n",
    "    \n",
    "    # Apply simpler Transformer block\n",
    "    x = simpler_transformer_block(x, node_size=32)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(16, kernel_initializer='he_normal')(x)\n",
    "    x = LeakyReLU()(x)  # Use LeakyReLU activation\n",
    "    outputs = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "                  loss=RMSE,  # Use RMSE as loss function\n",
    "                  metrics=[RMSE])  # Use RMSE as a metric as well\n",
    "    return model\n",
    "\n",
    "# Initialize the simpler transformer model\n",
    "simpler_transformer_model = build_simpler_transformer_model(X_train.shape[1])\n",
    "\n",
    "# Just a summary to visualize the architecture\n",
    "simpler_transformer_model.summary()\n",
    "\n",
    "# Define early stopping and learning rate reduction callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.009)\n",
    "\n",
    "# Train the simpler transformer model\n",
    "history = simpler_transformer_model.fit(X_train, y_train, epochs=400, \n",
    "                                        validation_data=(X_valid, y_valid), \n",
    "                                        batch_size=32, verbose=1,\n",
    "                                        callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the simpler transformer model on the validation set\n",
    "val_loss, val_mse = simpler_transformer_model.evaluate(X_valid, y_valid)\n",
    "print(\"Validation MSE:\", val_mse)\n",
    "\n",
    "# Predict on the test set\n",
    "predictions_simpler_transformer = simpler_transformer_model.predict(X_test_provided)\n",
    "\n",
    "# Create submission DataFrame for the simpler transformer model\n",
    "submission_simpler_transformer = pd.DataFrame({'Id': test_ids, 'SalePrice': predictions_simpler_transformer.squeeze()})\n",
    "submission_simpler_transformer.to_csv('Transformer_submission.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
